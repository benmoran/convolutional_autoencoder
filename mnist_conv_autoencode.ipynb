{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 870M (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "# add to kfkd.py\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from nolearn.lasagne import BatchIterator\n",
    "from theano.sandbox.neighbours import neibs2images\n",
    "from lasagne.objectives import squared_error as mse\n",
    "\n",
    "### this is really dumb, current nolearn doesnt play well with lasagne, \n",
    "### so had to manually copy the file I wanted to this folder\n",
    "from shape import ReshapeLayer\n",
    "\n",
    "from lasagne.nonlinearities import tanh\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import precision_score\n",
    "import os\n",
    "import urllib\n",
    "import gzip\n",
    "import cPickle\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Unpool2DLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer performs unpooling over the last two dimensions\n",
    "    of a 4D tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, incoming, ds, **kwargs):\n",
    "        \n",
    "        super(Unpool2DLayer, self).__init__(incoming, **kwargs)\n",
    "\n",
    "        if (isinstance(ds, int)):\n",
    "            raise ValueError('ds must have len == 2')\n",
    "        else:\n",
    "            ds = tuple(ds)\n",
    "            if len(ds) != 2:\n",
    "                raise ValueError('ds must have len == 2')\n",
    "            if ds[0] != ds[1]:\n",
    "                raise ValueError('ds should be symmetric (I am lazy)')\n",
    "            self.ds = ds\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "\n",
    "        output_shape[2] = input_shape[2] * self.ds[0]\n",
    "        output_shape[3] = input_shape[3] * self.ds[1]\n",
    "\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        ds = self.ds\n",
    "        input_shape = input.shape\n",
    "        output_shape = self.get_output_shape_for(input_shape)\n",
    "        return input.repeat(2, axis = 2).repeat(2, axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### when we load the batches to input to the neural network, we randomly / flip rotate the images, to artificially\n",
    "### increase the size of the training set\n",
    "\n",
    "class FlipBatchIterator(BatchIterator):\n",
    "\n",
    "    def transform(self, X1, X2):\n",
    "        X1b, X2b = super(FlipBatchIterator, self).transform(X1, X2)\n",
    "        X2b = X2b.reshape(X1b.shape)\n",
    "\n",
    "        bs = X1b.shape[0]\n",
    "        h_indices = np.random.choice(bs, bs / 2, replace=False) # horizontal flip\n",
    "        v_indices = np.random.choice(bs, bs / 2, replace=False) # vertical flip\n",
    "        \n",
    "        ###  uncomment these lines if you want to include rotations (images must be square)  ###\n",
    "        #r_indices = np.random.choice(bs, bs / 2, replace=False) # 90 degree rotation\n",
    "        for X in (X1b, X2b):\n",
    "            X[h_indices] = X[h_indices, :, :, ::-1]\n",
    "            X[v_indices] = X[v_indices, :, ::-1, :]\n",
    "            #X[r_indices] = np.swapaxes(X[r_indices, :, :, :], 2, 3)\n",
    "        shape = X2b.shape\n",
    "        X2b = X2b.reshape((shape[0], -1))\n",
    "\n",
    "        return X1b, X2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = 'mnist/mnist.pkl.gz'\n",
    "if not os.path.isfile(fname):\n",
    "    testfile = urllib.URLopener()\n",
    "    testfile.retrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", fname)\n",
    "f = gzip.open(fname, 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "X, y = train_set\n",
    "X = np.rint(X * 256).astype(np.int).reshape((-1, 1, 28,28)) # convert to (0,255) int range (we'll do our own scaling)\n",
    "mu, sigma = np.mean(X.flatten()), np.std(X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X.astype(np.float64)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "# we need our target to be 1 dimensional\n",
    "X_out = X_train.reshape((X_train.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 537257 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name          size\n",
      "---  ------------  --------\n",
      "  0  input         1x28x28\n",
      "  1  conv          32x22x22\n",
      "  2  pool          32x11x11\n",
      "  3  flatten       3872\n",
      "  4  encode_layer  40\n",
      "  5  hidden        9248\n",
      "  6  unflatten     32x17x17\n",
      "  7  unpool        32x34x34\n",
      "  8  deconv        1x28x28\n",
      "  9  output_layer  784\n",
      "\n",
      "  epoch    train loss    valid loss    train/val  dur\n",
      "-------  ------------  ------------  -----------  ------\n",
      "      1       \u001b[36m0.42700\u001b[0m       \u001b[32m0.20972\u001b[0m      2.03602  54.44s\n",
      "      2       \u001b[36m0.17209\u001b[0m       \u001b[32m0.14542\u001b[0m      1.18338  54.48s\n",
      "      3       \u001b[36m0.13335\u001b[0m       \u001b[32m0.12315\u001b[0m      1.08288  54.33s\n",
      "      4       \u001b[36m0.11775\u001b[0m       \u001b[32m0.11157\u001b[0m      1.05535  54.34s\n",
      "      5       \u001b[36m0.10948\u001b[0m       \u001b[32m0.10576\u001b[0m      1.03519  54.33s\n",
      "      6       \u001b[36m0.10477\u001b[0m       \u001b[32m0.10143\u001b[0m      1.03287  54.35s\n",
      "      7       \u001b[36m0.10105\u001b[0m       \u001b[32m0.09870\u001b[0m      1.02378  54.37s\n",
      "      8       \u001b[36m0.09829\u001b[0m       \u001b[32m0.09669\u001b[0m      1.01661  54.43s\n",
      "      9       \u001b[36m0.09602\u001b[0m       \u001b[32m0.09425\u001b[0m      1.01871  54.34s\n",
      "     10       \u001b[36m0.09416\u001b[0m       \u001b[32m0.09262\u001b[0m      1.01666  54.36s\n",
      "     11       \u001b[36m0.09273\u001b[0m       \u001b[32m0.09150\u001b[0m      1.01340  54.49s\n",
      "     12       \u001b[36m0.09127\u001b[0m       \u001b[32m0.08993\u001b[0m      1.01489  54.45s\n",
      "     13       \u001b[36m0.09023\u001b[0m       \u001b[32m0.08880\u001b[0m      1.01615  54.24s\n",
      "     14       \u001b[36m0.08896\u001b[0m       \u001b[32m0.08804\u001b[0m      1.01045  54.31s\n",
      "     15       \u001b[36m0.08816\u001b[0m       \u001b[32m0.08664\u001b[0m      1.01747  54.25s\n",
      "     16       \u001b[36m0.08753\u001b[0m       \u001b[32m0.08653\u001b[0m      1.01154  54.28s\n",
      "     17       \u001b[36m0.08619\u001b[0m       \u001b[32m0.08512\u001b[0m      1.01255  54.24s\n",
      "     18       \u001b[36m0.08599\u001b[0m       \u001b[32m0.08476\u001b[0m      1.01449  54.31s\n",
      "     19       \u001b[36m0.08481\u001b[0m       \u001b[32m0.08387\u001b[0m      1.01121  54.26s\n",
      "     20       \u001b[36m0.08407\u001b[0m       \u001b[32m0.08327\u001b[0m      1.00969  54.24s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_filters = 32\n",
    "deconv_filters = 32\n",
    "filter_sizes = 7\n",
    "epochs = 20\n",
    "encode_size = 40\n",
    "ae = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv', layers.Conv2DLayer),\n",
    "        ('pool', layers.MaxPool2DLayer),\n",
    "        ('flatten', ReshapeLayer), # output_dense\n",
    "        ('encode_layer', layers.DenseLayer),\n",
    "        ('hidden', layers.DenseLayer), # output_dense\n",
    "        ('unflatten', ReshapeLayer),\n",
    "        ('unpool', Unpool2DLayer),\n",
    "        ('deconv', layers.Conv2DLayer),\n",
    "        ('output_layer', ReshapeLayer),\n",
    "        ],\n",
    "    input_shape=(None, 1, 28, 28),\n",
    "    conv_num_filters=conv_filters, conv_filter_size = (filter_sizes, filter_sizes), \n",
    "    #conv_pad=\"same\",\n",
    "    conv_nonlinearity=None,\n",
    "    pool_pool_size=(2, 2),\n",
    "    flatten_shape=(([0], -1)), # not sure if necessary?\n",
    "    encode_layer_num_units = encode_size,\n",
    "    hidden_num_units= deconv_filters * (28 + filter_sizes - 1) ** 2 / 4,\n",
    "    unflatten_shape=(([0], deconv_filters, (28 + filter_sizes - 1) / 2, (28 + filter_sizes - 1) / 2 )),\n",
    "    unpool_ds=(2, 2),\n",
    "    deconv_num_filters=1, deconv_filter_size = (filter_sizes, filter_sizes), \n",
    "    #deconv_pad=\"same\",\n",
    "    deconv_nonlinearity=None,\n",
    "    output_layer_shape = (([0], -1)),\n",
    "    update_learning_rate = 0.01,\n",
    "    update_momentum = 0.975,\n",
    "    batch_iterator_train=FlipBatchIterator(batch_size=128),\n",
    "    regression=True,\n",
    "    max_epochs= epochs,\n",
    "    verbose=1,\n",
    "    )\n",
    "ae.fit(X_train, X_out)\n",
    "print \n",
    "###  expect training / val error of about 0.087 with these parameters\n",
    "###  if your GPU not fast enough, reduce the number of filters in the conv/deconv step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "pickle.dump(ae, open('mnist/conv_ae.pkl','w'))\n",
    "#ae = pickle.load(open('mnist/conv_ae.pkl','r'))\n",
    "ae.save_params_to('mnist/conv_ae.np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train_pred = ae.predict(X_train).reshape(-1, 28, 28) * sigma + mu\n",
    "X_pred = np.rint(X_train_pred).astype(int)\n",
    "X_pred = np.clip(X_pred, a_min = 0, a_max = 255)\n",
    "X_pred = X_pred.astype('uint8')\n",
    "print X_pred.shape , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAABwCAAAAAAiICN+AAAF1UlEQVR4nO3ca6imVRUH8N8575wz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###  show random inputs / outputs side by side\n",
    "\n",
    "def get_picture_array(X, index):\n",
    "    array = X[index].reshape(28,28)\n",
    "    array = np.clip(array, a_min = 0, a_max = 255)\n",
    "    return  array.repeat(4, axis = 0).repeat(4, axis = 1).astype(np.uint8())\n",
    "\n",
    "def get_random_images():\n",
    "    index = np.random.randint(5000)\n",
    "    print index\n",
    "    original_image = Image.fromarray(get_picture_array(X, index))\n",
    "    new_size = (original_image.size[0] * 2, original_image.size[1])\n",
    "    new_im = Image.new('L', new_size)\n",
    "    new_im.paste(original_image, (0,0))\n",
    "    rec_image = Image.fromarray(get_picture_array(X_pred, index))\n",
    "    new_im.paste(rec_image, (original_image.size[0],0))\n",
    "    new_im.save('data/test.png', format=\"PNG\")    \n",
    "    \n",
    "get_random_images()\n",
    "IPImage('data/test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## we find the encode layer from our ae, and use it to define an encoding function\n",
    "\n",
    "encode_layer_index = map(lambda pair : pair[0], ae.layers).index('encode_layer')\n",
    "encode_layer = ae.get_all_layers()[encode_layer_index]\n",
    "\n",
    "def get_output_from_nn(last_layer, X):\n",
    "    indices = np.arange(128, X.shape[0], 128)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # not splitting into batches can cause a memory error\n",
    "    X_batches = np.split(X, indices)\n",
    "    out = []\n",
    "    for count, X_batch in enumerate(X_batches):\n",
    "        #out.append(last_layer.get_output_for (X_batch).eval())\n",
    "        out.append(lasagne.layers.get_output(last_layer, X_batch).eval())\n",
    "        sys.stdout.flush()\n",
    "    return np.vstack(out)\n",
    "\n",
    "\n",
    "def encode_input(X):\n",
    "    return get_output_from_nn(encode_layer, X)\n",
    "\n",
    "X_encoded = encode_input(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAD5klEQVR4nO2aPWsUYRDHf3u3MXe+\nJGoCQUgiKogRlYCFWNiIhY2VlY32gt/Ej2BhoYiVWllY2NjEQgs1hoAEjaIYQ0CTXMzdJRYzk53b\n3ZMI7q2sO7Ds3POyz978d16fJ2AbFOi1qVecqtq+0WWun1PZzoJ/k4q/YBhf3ct70909jn5sqHzL\nzYuP9e3FF2nPFwyqQPsPJ4WI7oVADWgAPxE99Lil4Vh8kfYew24dFe1Mw7cPwauGYLQa66sgmPpF\nAp1TfJHmi2EcN5N9FbGVab7QqKZjW3pfj/Wb7S2+SPPzhwamj0s2Efy8TdxJp97VgQvAeWAaeAgs\nkIyDNij9YUa0hWGVpD+DpE6tIzq3pr/3A5eBa8B94LG2G/7eFpd6mAmFEOma5QiGY1XfaBcwgujf\nF8Re1pFY5geCIyRzi7RcpPgizU8PTQe9zKtIbHIIuI7o301g1o05BhzReR/p1NnQPdd8bfFFmg+G\naTYUIjwmgRvKvwJWEBs5AlwBjmvfV6CpvMVGFX2+6WjxRZoPhml5nPd5Z117E7GlDeAwcEbbPwEf\ntL+G/JMmSd0uvkjz9Ydt36hUBy4q/wx4ASwjejWC4AjwGnijfQOIDjfpTF5KW5oJhZbT+ZVbRLWz\n08BR5W8Bz4E9wCDiJ0fcnGXlLTaCZO2u+CLtPYYmY8vf4jZ1Uu/TwCPlLRbdR1R7GQMOAO/p/CZ8\nflHqYSYU+jq35XQ1JG4JgBPat4jEn3PASWBC+5aAIcSmTgJv9Rk7EJ8Zr9UVX6T5+MNuMccokjsA\nnANuI5hZ3jiB2FSQ3HGvXit0r8sVX6S9x9DH/Jbn21usAt/cYNPJJWCeKF75DjwBXur4FZLFdMsx\nii/SfHN8ENtnOrQI3EMwG0V0rELkD+tIXn8XuAPMEMVC8T0oo+KL9N/Ye+pDwG0gNe6a408Bl4Cr\niB+cUn5WH7YbwThtkTIuzYS2Yhrbqze+gsQlq0T7FA3gKTCM1LmHkNrMvPYPkdxzgjLHz5jCeOxh\nOrPq+D6gnyj/m0H2l8aRONXqOQ3EF3ajCv+DSPPF0Pi4T2shOBoNI7kgwGc3ruHGpJ0DKPPDTChM\ny+vjFD9vMU5Un6m79j4E7zbR3r+31WVcmglt6aGP/Q1X2xduI3HNABKPHkTwqSo/jtS7DT+I4luz\nny3KmCYj6tDDtPMYIVECsobYyzngARLzTBGdfzL87DvwbYHr6ykVf8HAMLSVu50D9jSG1NYWgHdE\nGJouB+6y76HEMDMKPLOZwnejQSRObROdRzS7Gz+TUaHcA86QEjm+32eo6v13Z4j7kbdeozPHtAfH\nc87ii7TnC/4ChAD+eWFnatQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_layer = ae.get_all_layers()[encode_layer_index + 1]\n",
    "final_layer = ae.get_all_layers()[-1]\n",
    "new_layer = layers.InputLayer(shape = (None, encode_layer.num_units))\n",
    "\n",
    "# N.B after we do this, we won't be able to use the original autoencoder , as the layers are broken up\n",
    "next_layer.input_layer = new_layer\n",
    "\n",
    "def decode_encoded_input(X):\n",
    "    return get_output_from_nn(final_layer, X)\n",
    "\n",
    "X_decoded = decode_encoded_input(X_encoded) * sigma + mu\n",
    "\n",
    "X_decoded = np.rint(X_decoded ).astype(int)\n",
    "X_decoded = np.clip(X_decoded, a_min = 0, a_max = 255)\n",
    "X_decoded  = X_decoded.astype('uint8')\n",
    "print X_decoded.shape\n",
    "\n",
    "### check it worked :\n",
    "\n",
    "pic_array = get_picture_array(X_decoded, np.random.randint(len(X_decoded)))\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('data/test.png', format=\"PNG\")  \n",
    "IPImage('data/test.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAEdklEQVR4nO3aS4hcVRAG4K+nZ6bH\njHlMMESN+CAocaNojIIguFCDgqAS1I1BcKEILlzp1qUuXYkQUER3goIg7nyC74gPFAPJwihGEo3m\nNe92UefMPX0zERfpbrjTBZfb957Xnfrnr1NVp1rOo4xjOV3nkrHzueD/keYv2FrtC+oYlJ3aqc84\nllSYLdXmyLKcnrurtA1Emr9gq63Sfyt9wVKt00YsYFYvvi2B5QacxFx6Xkzt7XRfTn3z/AOV5i/Y\nKn90i3uWCwUms+l5o8CqI3A9nd6PYQrrBJ65/0Tqp+g3UGn+gq1zYTcm8FgQmMFt2I0tmMHveBVf\n1SbNOM/W3o942Bdp1bHLsl7w70x6vg9P4mZhL6fT+6/xDj7Fu8X4C4qxK4tZCyodvk+TfRYqG/gw\nnsJOHMZ3aeAVuAyTOITX8ZrgZ31OYp9tvkoHvuB4/cWSyqcZwyMCv8vxFt7A98LObhO8fBDXYweu\nwT7BSwLrtur/ofkqHT6GpezB07gSL+MFHE1tLRzAZ9iPR3EP9go/6JTga95rJzBvLah0OLY074mT\nQs9wLx7HdryNZ4tBmwSvllV7XgfP4Zn0/Hx6zu057my+SgfPw9KnyfZul+DgerHHvVgMuFTY2uMC\n+4sEN+cEZptxP27AjfgkjctxZfNVOlwMuwK3XQKbD0Xs8E9q35D6HCn6dwWuvwnOvSR80i3YusqC\nzVfp4DHM/Mj5mo0CvyP4Er+m9o6INY7WJjgmcM/jfxLcu0lvbN8x2g/7JOOETcy+6Izg0xfCX8kY\nL6ti/XUCz7x3nhQ5mjMib3NY+DWHUnuOC7vWgkoHj2GZ4+6o8i/79cZ5Jac2iRzMKRWuOQacFna0\nq+JsV4V381U6PFtK2NFL0lfMF+/bAofcd1HvWcWEKs+9WcQiYyrcJ9N9FOP3RcYJjJYFDwke7RB7\n4t8Cv2mcSO1/CszyGRSB16Twb7amsWWubSH1b75Kh4ch4Vt+K3IytwqsPhb4nigGLaYxM4KDf6g4\nuUPk346LPfK0wC/vhc1X6XAwzPpfElhdjNuFX3JQYJsl51w2p/YzxfjtuBtXCX9orLgWjWxpn2QF\nw2wT28Jf2SDwOC5ixAOpPfsnR5yd234Id+IHvCd8mrZqr2QtqHR4MX7m0i94U8R8e/AEbsH7+EjE\nf3+p/JX1uA4P4DHBzc/FmRRn13U0X6WDx3Bc7/nhotD/gogdduOOdP0o4vdjAvMpXCtyajkn84o4\n3yi5V0rzVTr4nHf2LeeF3euo6is2ibznXeIMatt/TPSNyK3uEzylt7aG3rPJgUnzF2zV69g6wmc5\nWeu4U3DyahE/zAguHhR29gP8XE6swrBllPPuo6zUtZW579K+TqjqYtoinz0rvrSTxmberkyqyq9m\nHo7OnvomK+eHGcN6bc2UwHROdR5fr10ksJ5KbXN68zrEXzaqp+mL9NTTlHUvpeRc2oTIxeTz3wm9\nObclVSyxWp0Va0Glw8WwVdzz79U4l6WM31ebp6x5zPfmq3TgC/4LKrUTL58N47QAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_array = get_picture_array(X_decoded, 39823)\n",
    "image = Image.fromarray(pic_array)\n",
    "IPImage('data/test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
